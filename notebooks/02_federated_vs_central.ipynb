{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Federated vs Central Training Comparison\n",
    "\n",
    "This notebook loads metrics from central and federated GNN training,\n",
    "plots convergence curves, and compares performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "METRICS_DIR = Path(\"..\") / \"results\" / \"metrics\"\n",
    "FIGURES_DIR = Path(\"..\") / \"results\" / \"figures\"\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load central GNN training history\n",
    "history_path = METRICS_DIR / \"gnn_training_history.json\"\n",
    "if history_path.exists():\n",
    "    history = json.load(open(history_path))\n",
    "    print(f\"Central training: {len(history['train_loss'])} epochs\")\n",
    "    print(f\"Best val F1: {max(history['val_f1']):.4f}\")\n",
    "else:\n",
    "    print(\"No central training history found. Run scripts/run_all.py first.\")\n",
    "    history = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot central training curves\n",
    "if history:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    ax1.plot(history['train_loss'], 'b-o', markersize=3)\n",
    "    ax1.set_xlabel('Epoch'); ax1.set_ylabel('Loss'); ax1.set_title('Central GNN - Training Loss')\n",
    "    ax2.plot(history['val_f1'], 'g-o', markersize=3, label='F1')\n",
    "    ax2.plot(history['val_roc_auc'], 'r-s', markersize=3, label='ROC-AUC')\n",
    "    ax2.set_xlabel('Epoch'); ax2.set_ylabel('Score'); ax2.set_title('Central GNN - Validation Metrics')\n",
    "    ax2.legend()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(FIGURES_DIR / 'central_training_curves.png', dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FL round metrics\n",
    "fl_path = METRICS_DIR / \"fl_rounds.json\"\n",
    "if fl_path.exists():\n",
    "    fl_data = json.load(open(fl_path))\n",
    "    rounds = fl_data.get('rounds', [])\n",
    "    comm = fl_data.get('comm_bytes', [])\n",
    "    print(f\"FL rounds: {len(rounds)}\")\n",
    "    if rounds:\n",
    "        print(f\"Final round metrics: {rounds[-1]}\")\n",
    "    print(f\"Total communication: {sum(comm) / 1e6:.2f} MB\")\n",
    "else:\n",
    "    print(\"No FL metrics found. Run federated training first.\")\n",
    "    rounds, comm = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot FL convergence and communication cost\n",
    "if rounds:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    r_nums = [r.get('round', i+1) for i, r in enumerate(rounds)]\n",
    "    f1s = [r.get('f1', 0) for r in rounds]\n",
    "    ax1.plot(r_nums, f1s, 'g-o')\n",
    "    ax1.set_xlabel('FL Round'); ax1.set_ylabel('F1'); ax1.set_title('Federated GNN - F1 per Round')\n",
    "    if comm:\n",
    "        ax2.bar(range(1, len(comm)+1), [c/1e6 for c in comm])\n",
    "        ax2.set_xlabel('Round'); ax2.set_ylabel('MB'); ax2.set_title('Communication Cost per Round')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(FIGURES_DIR / 'federated_convergence.png', dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side model comparison table\n",
    "results = {}\n",
    "for name in ['rf', 'mlp', 'central_gnn', 'federated_gnn']:\n",
    "    p = METRICS_DIR / f'{name}_metrics.json'\n",
    "    if p.exists():\n",
    "        results[name] = json.load(open(p))\n",
    "\n",
    "if results:\n",
    "    print(f\"{'Model':<22} {'Precision':>10} {'Recall':>10} {'F1':>10} {'ROC-AUC':>10} {'Inf(ms)':>10}\")\n",
    "    print('-' * 75)\n",
    "    for name, m in results.items():\n",
    "        print(f\"{name:<22} {m.get('precision',0):>10.4f} {m.get('recall',0):>10.4f} \"\n",
    "              f\"{m.get('f1',0):>10.4f} {m.get('roc_auc',0):>10.4f} {m.get('inference_ms',0):>10.2f}\")\n",
    "else:\n",
    "    print('No metrics found yet.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart comparison\n",
    "if results:\n",
    "    models = list(results.keys())\n",
    "    metrics_to_plot = ['precision', 'recall', 'f1', 'roc_auc']\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.2\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    for i, m in enumerate(metrics_to_plot):\n",
    "        vals = [results[n].get(m, 0) for n in models]\n",
    "        ax.bar(x + i*width, vals, width, label=m.upper())\n",
    "    ax.set_xticks(x + width*1.5)\n",
    "    ax.set_xticklabels([n.replace('_',' ').title() for n in models], rotation=15)\n",
    "    ax.set_ylim(0, 1.05); ax.legend(); ax.set_title('Model Comparison')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(FIGURES_DIR / 'model_comparison_bar.png', dpi=150)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10"}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
